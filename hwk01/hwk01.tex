% Another example tex file
% Oberlin College
% Isaac Hollander McCreery

\documentclass[11pt]{amsart}

% document information
\newcommand{\hwk}{1}
\newcommand{\honor}{We affirm that we have adhered to the honor code on this assignment.}

\input{preamble}

\begin{document}
\maketitle

\section*{Part 1: Search Problem Formulation}

\begin{enumerate}

\item

\begin{description}

    \item[State space] We have 4 locations $a,b,c,d$ and let $L=\{a,b,c,d\}$ where
    $a:=\text{the door },b:=\text{the refrigerator starting spot },c:=\text{the papasan starting
    spot }, d:=\text{the closet.}$ Our state space will be $L \times L\times L$ where the ordered
    triple $(x,y,z)$ represents the location of the person, of the fridge, and of the papasan.

    \item[Start state]  The start state is $(a,b,c)$ which signifies that the person is at
    the door, the fridge is in its start location and the papasan is in its start location.

    \item[Available actions and successor function] There are two types of available
    actions.  The person walks from one location to another or the person moves either the fridge or
    papasan from one location to another.  The current state does not matter   if the person is
    merely walking from one location to another.  From any state $(x,y,z)$, the state $(x',y,z)$ is
    adjacent given any $x' \in L$. The person can move an item if the location of the person and the
    item are the same.   For     any state $(x,x,z)$ we can move to any state $(x',x',z)$ for any
    $x' \in L$ and likewise we can move from any state $(x,y,x)$ to any state $(x',y,x')$.

    \item[Goal test] Our goal test is to check if we are at state $(d,d,d)$.

    \item[Cost function] The optimal path would be to go to the fridge and move it and then
    get the papasan and move it and put it on the fridge.  So, our cost function should have an
    increased cost for moving the papasan before the fridge and also should have a higher cost for
    moving the items anywhere besides to the closet.  The cheapest edges should be the ones which
    represent moving the fridge first and then the papasan to the clost.

\end{description}

\item

\begin{description}

    \item[State space] The state space is the set of all actors, where each actor has a
    state and each pair of actors who were in a movie together have two directed edges between them,
    one going each way.  The edges of this state space will be updated as the game is played.

    \item[Start state] In addition, there will be a start state which does not represent
    any actor, but has an outward edge to every single vertex in the state space.  This represents
    the start of the game.

    \item[Available actions] A player can name an actor who starred in a movie with the
    last named actor.  At the beginning of a person's turn, the current vertex in the graph will
    correspond to the last named actor.  When an actor is named, all edges heading into it will be
    removed from the graph. The player whose turn it is must name an actor such that that named
    actor's vertex is adjacent to the current actor and that edge goes from the current actor to the
    new actor.

    \item[Successor function] Given a newly named actor.  If the actor is a valid choice,
    the vertex corresponding to that actor becomes the new current vertex and all edges heading into
    it will be removed from the graph.  If the choice isn't valid then the player loses or gets to
    go again depending how friendly the game is.  

    \item[Goal test] If a player gives a valid answer, and that answer's vertex has no
    outbound edges, that player wins because the next player has no valid guesses.

    \item[Cost function] There is no need for a cost function, if the only goal is to be
    able to check to see if one player has won the game.  If the point is to give strategy for
    playing the game, a cost function will not be the best way to do so because the cost function
    would have to constantly update and a certain move could be good or bad depending on the
    opponents next move.

\end{description}

\item

\begin{description}

\item[State space] $\{(a,b) \mid a \in \{0, \dots, 3\}, b \in \{0, \dots, 5\} \}$, where $a$ represents the amount of water in the 3-gallon container, and $b$ the 5-gallon.

\item[Start state] $(0, 0)$

\item[Available actions and successor functions] from $(a, b)$:
\begin{align*}
	(a, b) &\rightarrow (0, b), \text{ for emptying $a$} \\
	(a, b) &\rightarrow (a, 0), \text{ for emptying $b$} \\
	(a, b) &\rightarrow (3, b), \text{ for filling $a$} \\
	(a, b) &\rightarrow (a, 5), \text{ for filling $b$} \\
	\text{if $a + b \geq 3$}, (a, b) &\rightarrow (3, b - (3 - a)) \text{ for pouring $b$ into $a$ until $a$ is full} \\
	\text{if $a + b < 3$}, (a, b) &\rightarrow (a+b, 0) \text{ for emptying $b$ into $a$} \\
	\text{if $a + b \geq 5$}, (a, b) &\rightarrow (a - (5 - b), 5) \text{ for pouring $a$ into $b$ until $b$ is full} \\
	\text{if $a + b < 5$}, (a, b) &\rightarrow (0, a+b) \text{ for emptying $a$ into $b$}
\end{align*}

\item[Goal test] is $b = 4$?

\item[Cost function] uniform cost; any action costs 1

\end{description}

\item

\begin{description}

% TODO do we need to give more explanation?

\item[State space] $\{(t, m, b, f) \mid t, m, b, f \in \{0, 1\}\}$, where $t,
m, b, f$ represent the T-rex, monkey, banana, and farmer, respectively; 0 means the actor is on the
start shore, 1 represents the opposite shore.

\item[Start state] $(0, 0, 0, 0)$

\item[Available actions and successor functions] from $(t, m, b, f)$:
\begin{align*}
	&\text{ invert $f$ (change it from 0 to 1 or from 1 to 0), for the farmer crossing by himself} \\
	\text{if } t = f, &\text{ invert both $t$ and $f$, for the farmer taking the T-rex across} \\
	\text{if } m = f, &\text{ invert both $m$ and $f$, for the farmer taking the monkey across} \\
	\text{if } b = f, &\text{ invert both $b$ and $f$, for the farmer taking the banana across}
\end{align*}

\item[Goal test]: is the state $(1, 1, 1, 1)$?

\item[Cost function]: $\infty$ if action a results in a state $(t, m, b, f)$ where $t
= m \neq f$ or where $m = b \neq f$, 1 otherwise; leaving the T-rex and monkey together or the
monkey and banana together will result in losing the monkey or the banana.

\end{description}

\end{enumerate}

\section*{Part 2: Heuristic Functions}

\emph{Theorem.}
If a heuristic is consistent, then it must be admissible.

\begin{proof}
Suppose that the heuristic $h: Q \rightarrow \mathbb{R}^+$, where $Q$ is the state space for a given
problem, is consistent.  Then for any $n \in Q$, $h(n) \leq c(n, a, n') + h(n')$ for all successors
$n'$ of $n$, where $c(n, a, n')$ is the cost of moving from $n$ to $n'$.  But, similarly, $h(n') \leq
c(n', a, n'') + h(n'')$ for all successors $n''$ of $n'$.  We may continue in this fashion until we
get to some successor $n^k$ that is a goal state, and whose heuristic is zero:
\[
h(n) \leq c(n, a, n') + h(n') \leq c(n, a, n') + c(n', a, n'') + h(n'') \leq \dots \leq
\sum_{i = 1}^k c(n^{i-1}, a, n^i),
\]
but one of such sums is the minimum cost path from $n$ to a goal state $n^k$, so
\[
h(n) \leq \sum_{i = 1}^k c(n^{i-1}, a, n^i) = h^*(n),
\]
and thus $h$ is admissible.
\end{proof}

\emph{Theorem.}
If a heuristic is admissible, then it is not necessarily consistent.

\begin{proof}
Consider the following example:

\vspace*{1in}

where $h(n) = 3$ and $h(n') = 1$.  Then $h(n) = 3 = h^*(n)$ and $h(n') = 1 \leq 2 = h^*(n')$,
so $h$ is admissible.  However, $h(n) = 3 \nleq 2 = c(n, a, n') + h(n')$, so $h$ is not consistent.
\end{proof}

\section*{Part 3: Advanced Search}

\subsection*{Hill-climbing}  Given a graph $G$, a state $s \subseteq V(G)$ is some clique in $G$
with at least one vertex.  A neighbor of $s$ is any $s' = s \cup \{y\} \subseteq V(G)$ where $y$ is a
neighbor of all vertices in $s$.

Start at $s = \{x\}$ for some random vertex $x \in V(G)$.  Move to any neighbor $s'$ by adding a
vertex $y$ that is a neighbor of all vertices in $s$.  Repeat until there is no neighbor $y$ to add,
and return.

\subsection*{Simulated annealing}
We can approximate the best maximum clique in a graph $G$ with this simulated
annealing algorithm.  First, select a single vertex $v \in G$ and let the list
$C = \{v\}$ and let the list $Q = \{u \mid u \text{ is adjacent to all vertices
in } C\}$.  $C$ is the list representing the clique and $Q$ is the list which
represents all vertices which may be added to the clique.  For each iteration
of the algorithm, select a random vertex $v \in C \cup Q$.  If $v$ is in $Q$,
remove $v$ from $Q$ and add it to $C$ and then update $Q$ to account for $v$
now being in $C$.  If $v$ is in $C$, with probability $e^{((|C|-1)-|C|)/T} =
e^{-1/T}$ remove $v$ from $C$ and add it to $Q$ and then update $Q$
accordingly.  At the end of the algorithm, return $C$.  

In this algorithm we build a clique starting out from a single vertex.  We add
vertices to the clique that can be added while the clique remains a clique and
we, with some probability, remove vertices already placed in the clique.  By
being able to remove vertices from the clique we are able to backtrack out of
local minimums.  For variations on this, you could build a priority queue for
$Q$ instead of just a list of vertices, and you could select the next vertex to
add (or remove) based on the number of edges (or some other criterion) incident
to that vertex.  The probability is based on the function, given a state $s$,
$f(s) = |C|$.  Thus if we are considering removing some vertex, $f(next) = |C|
- 1$, and that is how the probability above was defined.

\subsection*{Genetic}
This genetic algorithm is not guaranteed to find a clique and likely won't.
Instead, we came up with a way of rating a graph's \emph{cliqueness}. If a
clique and had $n$ vertices, then it would necessarily have $\frac{n(n-1)}{2}$
edges. Any fewer edges means its cliqueness is less. Therefore, if a graph has $e$
edges, then a measure of its cliqueness could be $\frac{2e}{n(n-1)}$. This would be 1 if
the graph is a clique and less otherwise. To favor large cliques, our fitness
function for a given graph or sub-graph will be
\[
f(G) = \Bigg(\frac{2e}{n(n-1)}\Bigg)n.
\]

Given a graph, start with $x$ random subgraphs. Compute their fitness.
Select the two fittest subgraphs. % TODO this should be probabalistic
Mate them by selecting a random subset of the union of
the two parent subgraphs, and mutate by adding an additional random vertex.
Create $x$ off-spring this way, and repeat. Iterate until a subgraph of desirable
size and cliqueness (of desirable fitness) is found.

% TODO parameter variations

\section*{Part 4: AI in the World}

\subsection*{Ciphers}

A homophonic substitution cipher is an encryption which maps each plaintext
character in a message to one or more ciphertext symbols.  This type of
encryption is much more difficult to decode than a monoalphabetic cipher, which
is one in which each plain text letter is mapped to exactly one ciphertext
letter.  David Oranchak uses a genetic algorithm in order to decipher
homophonic substitution ciphers.  First, the algorithm finds the segment of the
cipher in which the highest concentration of different cipher-text letters are
(i.e.\ A portion such that (the number of unique ciphertext symbols in that
section divided by the number of total unique ciphertext symbols in the entire
message) is maximized). The population is a set of words from a dictionary
(i.e.\ the english language) and the selected high-concentration of the
cyphertext is matched up with these words, to check to see if it is a good
match.  Oranchak defined two fitness functions.  The first fitness function was
a measure of how many of the words outside of the high concentration section
are actual words in the dictionary.  The second fitness function is measured by
building a graph $G$ and letting each vertex be a found word in the cypher
text.  For every conflict, in which two words do not match due to the
cyphertext, an edge is placed between those vertices.  The second fitness
function is a measure of the number of edges between the vertices, with fewer
edges being proportional to higher fitness.  The merging process is done by
running tournament selection of size two, with a small probability of selecting
the less fit of the pair.  When parents merge, random character selections are
swapped and there is a small chance of mutation.  The algorithm precisely
decoded the Zodiac Killers homophonic substitution cipher.

\emph{Source.} Oranchak, David.  \emph{Evolutionary Algorithm for Decryption of
Monoalphabetic Homophonic Substitution Ciphers Encoded as Constraint
Satisfaction Problems.}

\subsection*{Using a genetic algorithm to determine StarCraft 2 build orders}

In the game StarCraft 2, before interacting with the "enemy" you as the player
have time to run a series of build orders.  Those build orders will determine
how many of all your things and people you have and will thus determine how you
are able to interact with your enemies.  In this genetic algorithm, the
population is a set of build orders and the fitness function is how close that
set of build orders gets you to some desired outcome.  This algorithm ignores
all sets of build orders with a fitness of 0, which implies that the build
orders are an invalid sequence.  The algorithm takes the most fit sets of build
orders and combines them by concatenating sub-sets of them together.  There is
also the chance of mutation, including insertion and deletion of a build order.
In the end, the algorithm spits out the most fit set of build orders.  One of
the pro's of this algorithm is that it might possibly give you an excellent
SC2 opening strategy, and you wouldn't have to think about it at all.
However, the fitness test is based upon reaching a "desired" state, which must
be a subjectively chosen state, which means that that state isn't globally
optimal.  This algorithm did come up with an extremely unorthodox strategy
though which turned out to be extremely effective and nearly unbeatable.

\emph{Source.} \texttt{http://lbrandy.com/blog/2010/11/using-genetic-algorithms-to-find-starcraft- 2-build-orders/}

\section*{Part 5: Project Prep}

% TODO these both probably need more (why is it interesting?  why use these search techniques?)

\subsection*{Search Search}
Given a certain search problem, what annealing function finds the best solution
on average? Fix the number of search steps to $X$. Then search for the optimal
function temperature $T(t)$ where $t \leq X$. Does a linearly decreasing temperature
tend to find the best solution for a given problem.  Asymptotically
decreasing?. The state space would be different functions $T(t)$ and fitness of
that state would be the average success of that function at finding a good
solution for a particular problem. This would likely require lots of
computation, perhaps over several days. The “search search” itself could be
hill climbing, annealing, or genetic. If the T functions are encoded as
$X$-tuples $(T_1, T_2, \dots, T_X)$, then two $T$-functions could be “mated” by
combining them step-wise at $X/2$.

This would be interesting because while the purpose of having a decreasing
temperature is clear, it is not so obvious why different rates of decrease
would be better suited to a certain problem.

\subsection*{Neural Networks}
Evolve (search for) a neural network for accomplishing simple yet novel task. A
neural network can be straightforwardly modeled by a matrix (or “connectrix”)
representing synapses between neurons (edges between vertices). The state space
would be huge. A 30-neuron network would be a matrix with 900 entries. Then,
each entry would have a range of values.

The fitness of a network would be it's average success (over a fixed number of
trials) at solving a particular kind of problem such as detecting whether a
picture is of a human being or a dog's face, though even that may be too
complicated. Simulated annealing may be the best search option\----an action could
be to add or remove or change the strength of a synapse, or the firing behavior
of a neuron.

Hill climbing would not allow for enough exploration, and genetic cross-overs
do not seem appropriate for this problem\----too much change at once.

\honor

\end{document}
