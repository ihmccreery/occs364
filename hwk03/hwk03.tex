% Another example tex file
% Oberlin College
% Isaac Hollander McCreery

\documentclass[11pt]{amsart}

% document information
\newcommand{\hwk}{3}
\newcommand{\honor}{We affirm that we have adhered to the honor code on this assignment.}

\input{preamble}

\begin{document}
\maketitle

\section*{Part 1: Probability Conundrum}

\begin{enumerate}

\item
\emph{Claim.}  
All prisoners were equally likely to be executed to begin with, and now it is just between $A$ and $C$; therefore, $A$
believes his probability of being executed is now $\frac{1}{2}$ after receiving the data that $B$ will be pardoned.

\emph{Discussion.}
Put more mathematically, we might write
\[
P(A | \neg B) = \frac{P(A \wedge \neg B)}{P(\neg B)} = \frac{\frac{1}{3}}{\frac{2}{3}} = \frac{1}{2}.
\]
The problem is that $A$ didn't receive the data that $B$ will be pardoned; $A$ received the data that \emph{the guard told
$A$ that $B$ would be pardoned}.

\item
\emph{Claim.}
$A$ had a $\frac{1}{3}$ chance before receiving the data, and nothing really has changed about the governor’s choice.
Therefore $A$’s probability of being executed remains at $\frac{1}{3}$.

\begin{proof}
Looking at all the possibilities, we have
\begin{align*}
A &\Rightarrow \underline{B} \vee \underline{C} \text{, each with equal probability}\\
B &\Rightarrow \underline{C} \\
C &\Rightarrow \underline{B},
\end{align*}
where $\underline{B}$ is that the guard tells $A$ that $B$ will be pardoned, and $\underline{C}$ is that the guard tells
$A$ that $C$ will be pardoned.  Then we have
\begin{align*}
P(\underline{B} \mid A) = P(\underline{C} \mid A) &= \frac{1}{2} \\
P(\underline{B}) =
P(A \wedge \underline{B}) + P(C \wedge \underline{B}) =
P(\underline{B} \mid A)P(A) + P(C \wedge \underline{B}) &= \frac{1}{3}\cdot\frac{1}{2} + \frac{1}{3} = \frac{1}{2}.
\end{align*}
So,
\[
P(A \mid \underline{B}) = \frac{P(\underline{B} \mid A)P(A)}{P(\underline{B})} = \frac{\frac{1}{2}\cdot\frac{1}{3}}{\frac{1}{2}} = \frac{1}{3}.
\]
\end{proof}

\end{enumerate}

\section*{Part 2: Bayes' Theorem}

Let $C$ be that Jack completed his homework, and let $F$ be that Jack says he forgot to print it.  Then we have
\begin{align*}
P(F \mid C) &= \frac{1}{100} \\
P(F \mid \neg C) &= \frac{1}{2} \\
P(C) &= \frac{9}{10}.
\end{align*}
So,
\begin{align*}
P(C \mid F) = \frac{P(F \mid C)P(C)}{P(F)} = \frac{\frac{1}{100}\cdot\frac{9}{10}}{P(F)} = \frac{\frac{9}{1000}}{P(F)} &=
\frac{9}{1000}\alpha \\
P(\neg C \mid F) = \frac{P(F \mid \neg C)P(\neg C)}{P(F)} = \frac{\frac{1}{2}\cdot\frac{1}{10}}{P(F)} =
\frac{\frac{1}{20}}{P(F)} &= \frac{1}{20}\alpha \\
\end{align*}
But
\begin{align*}
P(C \mid F) + P(\neg C \mid F) &= 1 \\
\frac{9}{1000}\alpha + \frac{1}{20}\alpha &= 1 \\
\frac{59}{1000}\alpha &= 1 \\
\alpha &= \frac{1000}{59}, \\
\end{align*}
so
\[
P(C \mid F) = \frac{9}{1000}\alpha = \frac{9}{1000}\cdot\frac{1000}{59} = \frac{9}{59}.
\]

\section*{Part 3: Bayesian Networks}

\begin{enumerate}

\item
\begin{align*}
P(A \wedge B \wedge C \wedge D \wedge E)
	&= P(A) P(B) P(C) P(D | A \wedge B) P(E | B \wedge C) \\
	&= 0.2 \times 0.5 \times 0.8 \times 0.1 \times 0.3 \\
	&= 0.0024
\end{align*}

\item
\begin{align*}
P(\neg A \wedge \neg B \wedge \neg C \wedge \neg D \wedge \neg E)
	&= P(\neg A) P(\neg B) P(\neg C) P(\neg D | \neg A \wedge \neg B) P(\neg E | \neg B \wedge \neg C) \\
	&= 0.8 \times 0.5 \times 0.2 \times 0.9 \times 0.7 \\
	&= 0.0504
\end{align*}

\item
\begin{align*}
P(\neg A \wedge B \wedge C \wedge D \wedge E)
	&= P(\neg A) P(B) P(C) P(D | \neg A \wedge B) P(E | B \wedge C) \\
	&= 0.8 \times 0.5 \times 0.8 \times 0.6 \times 0.3 \\
	&= 0.0576
\end{align*}

\end{enumerate}

\section*{Part 5: AI in the World}

\begin{description}

\item[Bayesian belief networks for adaptive management] Hans Jørgen Henriksen and Heidi Christiansen Barlebo cite that
Bayesian networks (BNs) are often helpful in highly complex problems where "there is a scarcity and uncertainty in the
data used in making the decision and the factors are interlinked" (1026).  BNs allow stakeholders to work out with
experts what causal links exist in a system in order to better develop models for possible outcomes.  In their work
with pesticide reduction instruments\----contracts with farmers\----and their impact on groundwater safety, the authors
found that "rather costly compensations" would be required to ensure a 95\% probability that the water resources would
remain safe (1029).  When a special uncertainty arose where stakeholders disagreed, an additional "perception of
vulnerability" state was added to address this disagreement rather than favor one group over another (1029).  In their
follow-up, the authors found that two water managers involved in the process thought that BNs provided an alternative to
standard welfare economics, as BNs "could help to delineate the complexities and also handle some of the uncertainties"
associated with water management (1031).

Source:
Henriksen, Hans Jørgen \& Heidi Christiansen Barlebo.
``Reflections on the use of Bayesian belief networks for adaptive management''.
\emph{Journal of Environmental Management}.
88 (2008) 1025-1036.

\item[Automated Essay Scoring Using Bayes' Theorem]
This research paper goes over a program which grades human-written essays.
Initially, the program was given four-hundred some papers that were
human-graded and the program was given those grades.  The program built
Bayesian models, a Bernoulli model and a multinomial model, using single words,
two-word pairs and arguments, where arguments are defined as one word (or
two-word) before another.  The idea was, for example, the word ``x'' occurs more
in good papers and so, when the word ``x'' is found, the chances of the paper
being a good paper is higher.  Likewise, an argument ``xy'' could be a sequence
which often occurs in bad papers and thus if a paper reads an ``x'' and then a
``y'', there is an increased probability that the paper is bad.  In the end,
given only 462 total human-graded papers as a base, the program was able to
grade papers at an accuracy of 81\% (this was figured as the papers were also
human-graded by a grader who did not know the program's grade and vice versa).

Source:
Rudner, Lawrence M. \& Tahung Liang.
``Automated Essay Scoring Using Bayes’ Theorem''.
\emph{The Journal of Technology, Learning, and Assessment}.
Volume 1, Number 2: June 2002.

\end{description}

\section*{Part 6: Project Prep}

\begin{description}

\item[Bayesian belief networks with the Oberlin Project's Green Arts District]
The Oberlin Project's proposed Green Arts District on the block just East of Tappan Square in Oberlin has been both
lauded and criticized for its innovation and impact.  Constructing a Bayesian belief network (BN) modeling the
construction of the District and its various impacts might provide a clearer vision of the inputs and results of such a
project.  In Henriksen and Barlebo's work above, the authors produced a BN through an involved stakeholder process to
allow stakeholders to agree upon the interrelations in a complex policy system, providing a precedent we might follow:
\renewcommand{\theenumi}{(\roman{enumi})}
\renewcommand{\labelenumi}{\theenumi}
\begin{inparaenum}
\item define the context;
\item identify factors, actions and indicators;
\item build pilot networks;
\item collect data;
\item define states;
\item construct CPTs; and
\item collect feedback from stakeholders.
\end{inparaenum}
\renewcommand{\theenumi}{\alph{enumi}.}
\renewcommand{\labelenumi}{\theenumi}
The results might be, among other data, predictions for the economic impact of the project\----income brought to
Oberlin, gentrification and changes real estate prices\----environmental impact of the project\----energy saved,
materials used\----and social impact\----recognition, leadership and proof-of-concept value, changes in tensions between
the Town and the College.

\item[Quotation and Exclamation]
Build a Bayesian network that models the probability that an excerpt of plain
text was part of a quotation (perhaps also whether it was italicized or in
bold\----anything boolean like that) in it's original context.  Along the same
lines, what words characterize sentences that end in exclamation points?  This
would be similar to the spam filter idea.  It could be accumulated by parsing
text from source (perhaps narrowing the input text would yield more interesting
results).  What we'd be able to answer: given an excerpt of text, what is the
probability that it was originally part of a quotation (i.e. between ``''
symbols).  What we wouldn't be able to do: given an excerpt of text that has
had the quotation marks removed, make a guess as to where quotation marks
should be (that would be more appropriate for hidden Markov stuff).  With
well-formed text, it is easy to know whether a word was in quotes (whether or
not you've seen an odd number of quotation marks so far\dots new paragraphs might
require special attention) or if the sentences ends with an exclamation point.
It seems like there could be practical application of the results, and if not,
it would still be interesting to see general trends that distinguish written
words from allegedly or hypothetically spoken words, and/or sentences that are
supposed to have extra ``enthusiasm''.  Non-practical application: given a
paragraph that ends only in boring periods, make it instantly more fun to read
by replacing some of the periods with exclamation marks based on conditional
probabilities.

\end{description}

\honor

\end{document}
